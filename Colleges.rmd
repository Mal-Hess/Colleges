---
title: "Analysis of College Tuitions"
author: "Malcolm Hess"
date: "Thursday, April 09, 2015"
output: html_document
---

Contributed by Malcolm Hess.
This is based on my project investigating college tuitions.


#Links
The code and files for this project can be found here: https://github.com/Mal-Hess/Colleges

If you enjoyed this or would like to see more of my work feel free to check out my github account for more projects. 
https://github.com/Mal-Hess/


#Background

There is increasing importance in our society to have a college degree from a top university.  However the costs for attending a university across the country have been drasticly increasing.  This has made going to college for some simply a dream and many that do go to college graduate with massive students debts.  This project is designed to analyze what variables if any effect college attendance costs and would be good predictors in estimating tuition. 

#Objective

+ Find a list of top colleges in the U.S. and download variables that can be used to analyze the cost of tuition.

+ Clean and organize the data.

+ Check for multi-colinearity of variables.

+ Build a linear model to see which varaibles if any are significant predictors of tuition cost.

+ Perform a PCA analysis of the variables and build a PCA regression model.


#Implementation

There are many different college ranking sites and after looking at many options I decided to use Forbe's list of top colleges. I used Forbe's list and data because I feel Forbes is a trustworthy source.

From the site I was able to get the names, ranks, and also links to the individual pages for each of the top 500 colleges on the Forbe's list.  Below you can see the link to the top colleges list.

http://www.forbes.com/top-colleges/

Forbes also has a dedicated page for every college in their list.  I web scraped data off of these dedicated pages to create my data frame.  Below you can see the link of what one of these individual sites looks like.

http://www.forbes.com/colleges/stanford-university/


#Conclusion

There are definitely some signficant variables that can be used to predict and estimate the price of a college.  Several commonly held beliefs are validated with this research.  Most students would rather go to a cheaper school but they are very willing to pay more in order to go to a more challenging, competetive, and desirable school. Unfortunately, more expensive universities also require students receive larger grants and take on bigger loans.     

The variables that came out significant in my linear model are:

+ %acceptedenrolled :Lower % accepted that enroll leads to higher costs. 

+ studenttofacultyratio :lower student to faculty ratio leads to increased costs. 

+ fouryeargrad : Schools with a higher four year graduation rate cost more than low graduation rate universities.

+ %onfinancialaid : As the cost of a school increases the amount of students that will be on financial aid increases.

+ averageloan : More expensive shcools means that students need bigger loans

+ averagegrant : More expensive schools leads to students needing larger grants

+ % athlete : Colleges with a lower % of student athletes cost more.


##Getting the Data

The list of top colleges is broken down so that there is a page per 100 colleges. I downloaded the first 5 pages manually.  Using x-paths I took out the names, links, and ranks for the top 500 colleges.  At the end I also saved that object and along with other objects to disk.  I have also uploaded these binary data files to my github page so anyone who wishes to recreate this project will not have to download the sites and web scrape them.

```{r, echo=FALSE, eval=TRUE}
require(XML)
require(RCurl)
library(rvest)
library(car)
library(pls)
```


```{r, echo=TRUE, eval=FALSE}
#name of downloaded sites that have Forbes' top colleges lists.
listofcolleges <- c("Forbes1.html", "Forbes2.html", "Forbes3.html", "Forbes4.html", "Forbes5.html")

topcolleges <- NULL

for(i in listofcolleges){
  
tables <- readHTMLTable(i)
df <- data.frame(tables[27])

parsed <- htmlParse(i)
xpath <- '//*[@id="listbody"]/tr/td/a/@href'
nodes <- getNodeSet(parsed, xpath)
links <- sapply(nodes, toString)


top <- cbind(df, links)
topcolleges <- rbind(topcolleges, top)
}

topcolleges <- topcolleges[,c(-4,-5)]
names(topcolleges) <- c("rank", "name", "state", "full_link")

topcolleges$full_link <- gsub("\\?list=top-colleges", "", topcolleges$full_link)


#saving topcolleges object:
#save(topcolleges, file="list of top colleges")
#load("list of top colleges")
```

The extractdata function has individual xpaths for all of the information I chose to remove from the individual college sites.  This method does require that the sites be uniformly made.  The only individual sites that are not uniform have no tuition cost.  The extractdata function will be run on those sites too but they will be removed later since they are of no use to my analysis.  There are 39 variables in total that are collected. I saved the object 'mat' which has all of these variables to disk.

Below are a few examples of how the extract data function works.  

```{r, echo=TRUE, eval=FALSE}
extractdata <- function(data) {

  vect <- c() 
  
  #total cost-in state
  xpath <- '//*[@id="tuition"]/ul/li/table[3]/tbody/tr[2]/td[2]'
  nodes <- getNodeSet(data, xpath)
  value <- sapply(nodes, xmlValue)
  if(!is.null(value)){
    vect[1] <- toString(value)
  }else {vect[1] <- NA}

  #total cost-out state
  xpath <- '//*[@id="tuition"]/ul/li/table[3]/tbody/tr[2]/td[3]'
  nodes <- getNodeSet(data, xpath)
  value <- sapply(nodes, xmlValue)
  if(!is.null(value)){
    vect[2] <- toString(value)
  }else {vect[2] <- NA}
  
    #Grand total sports revenues
  xpath <- '//*[@id="athletics"]/ul/li/table[4]/tbody/tr[5]/td[4]'
  nodes <- getNodeSet(data, xpath)
  value <- sapply(nodes, xmlValue)
 if(!is.null(value)){
   vect[39] <- toString(value)
 }else {vect[39] <- NA}
  
 vect <- gsub("\n", "", vect)
 vect <- gsub("\t", "", vect)
  
  vect
} #End of extract data function

#Download all of the individual college websites.
mat <- matrix(NA, nrow=500, ncol=39)
i <- 1
while(i <= nrow(topcolleges)){
  
  collegeurl <- topcolleges$full_link[i]
  rawcollege <- getURL(collegeurl)
  nameoffile <- paste0(toString(topcolleges$name[i]), ".Rdata")
  save(rawcollege, file=nameoffile)

  parsed <- htmlParse(rawcollege)
  mat[i,] <- extractdata(parsed)
  
i <- i + 1 
} #End of while loop

#save(mat, file="variables.Rdata")
#load("variables.Rdata")

```


##Cleaning the Data
There was plenty of cleaning to do.  Commas and dollar signs had to be removed.  Many of the variables I collected were percents, which needed to have the percent symbol removed and then be converted to the actual value.   


```{r, echo=TRUE, eval=FALSE}

mat$rank <- gsub("#", "", mat$rank)
# returns string w/o leading or trailing whitespace
trim <- function (x) gsub("^\\s+|\\s+$", "", x)
mat$rank <- trim(mat$rank)

topcolleges$rank <- sapply(topcolleges$rank, toString)

mat <- mat[order(mat$rank),]
topcolleges <- topcolleges[order(topcolleges$rank),]

final <- merge(topcolleges, mat, by="rank")

final <- sapply(final, function(x)sub("\\$","", x))
final[,5:42] <- sapply(final[,5:42], function(x)sub("\\,","", x))
final[,5:42] <- sapply(final[,5:42], function(x)sub("\\,","", x))

percents <- c(12,13,14,15,16,17,18,19, 20,21,22,23,24,25,29 ,33,34,41)

final[,percents] <- sapply(final[,percents], function(x)sub("\\%","", x))

arenumbers <- c(1, 5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,41,42)
arefactors <- c(1, 2,3,37,38,39,40)

final <- as.data.frame(final, stringsAsFactors = FALSE)
final[,arenumbers] <- sapply(final[,arenumbers], as.numeric)
final[,percents] <- sapply(final[,percents], function(x) x/100)

final <- final[order(final$rank),]
final[,arefactors] <- sapply(final[,arefactors], factor)


```


#Checking for Multicolinearity 

I checked both the covariance and correlation matrix to see if there was any multicolinearity in my data.  I expected that there would be more than a few since many of the variables are directly dependent on the same things, for example the cost of in-state schooling and out-of-state schooling. For most universities these are exactly the same; universities where these two costs differ are usually state or city funded schools. 

Below, I have highlighted some groups of variables which have a lot of multi-colinearity.  I used this information to perform some variable selection on my own by removing variables that had high correlation.  


```{r, echo=TRUE, eval=TRUE}
load("finaltable.Rdata")
costs <- c(5,8,10,11,15,16)

corr1 <- cor(final[,costs],  use = "complete", method = c("pearson", "kendall", "spearman"))

morecovariance <- c(6,32,33,34,35,36,41,42)

corr2 <- cor(final[,morecovariance],  use = "complete", method = c("pearson", "kendall", "spearman"))

corr1
corr2


```

The next step was to center and standardize all of the continous variables. They were scaled to have a mean zero and standard deviation of 1.  

```{r, echo=TRUE, eval=FALSE}
normality <- c()

withoutrank <- c(5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,41,42)


for(i in withoutrank){
  final[,i] <- scale(final[,i])
}

final$calendar <- sapply(final$calendar, as.factor)
#final$campus_housing <- sapply(final$campushousing, as.factor)
final$setting <- sapply(final$setting, as.factor)
final$religion <- sapply(final$religion, as.factor)
final$state <- sapply(final$state, as.factor)
final$rank <- sapply(final$state, as.numeric)

#save(final, file="finaltable.Rdata")

```

I checked the factor variables, such as calendar and setting, with boxplots.  None of them appeared to have significant differences between the groups.   

```{r, echo=TRUE, eval=TRUE}
load("finaltable.Rdata")
boxplot(totalcostinstate ~ calendar, data=final)
boxplot(totalcostinstate ~ setting, data=final)
```

#Building a Linear Model

I furthered my analysis by creating linear models.  I analyzed many and tried a multitude of different approaches particularly in how I paired variables together interaction terms.  Eventually I settled on a varaible combination I called basemodel.  


9 variables came out significant:  

- %acceptedenrolled
- studenttofacultyratio
- fouryeargrad 
- %onfinancialaid 
- averageloan
- averagegrant
- % athlete
- studenttofacultyratio:fouryeargrad
- averageloan:averagegrant 

```{r, echo=TRUE, eval=TRUE}
final_vars <- final[,c(1, 3, 5,7, 9, 12,13, 14,15,16,17,22,28,29,30,31,32,33,34,35,36,37,40,41,42)]

final_complete <- final_vars[!is.na(final_vars$totalcostinstate),]

allmodel <- lm(totalcostinstate ~ ., data=final_complete, na.action=na.omit)

nostate <- lm(totalcostinstate ~ . -state , data=final_complete, na.action=na.omit)

basemodel <- lm(totalcostinstate ~ rank + acceptedenrolled +  studentpopulation * amountofapplicants + submitttingsat +
                  studenttofacultyratio * fouryeargrad +  onfinancialaid * recevingloan + fulltime + female + white +
                  averageloan * averagegrant + applicationfee + state + 
                sat25 * admitted + calendar+ setting + athlete + sportsrevenue, data=final_complete, na.action=na.omit)

basenostate <- lm(totalcostinstate ~ rank + acceptedenrolled +  studentpopulation * amountofapplicants + submitttingsat +
                  studenttofacultyratio * fouryeargrad +  onfinancialaid * recevingloan + fulltime + female + white +
                  averageloan * averagegrant + applicationfee + 
                  sat25 * admitted + calendar+ setting + athlete + sportsrevenue, data=final_complete, na.action=na.omit)
```
```{r, echo=TRUE, eval=TRUE}
summary(allmodel)
summary(basenostate)

```

I tested the basenostate model against several other models including a backwards-step-wise model.  The basenostate model had one of the lowest AIC and therefore I accept it to be the best of the linear models that I have created.  

The VIF check was done to see how much multi-colinearity there might still be between the variables.  There is still quite a bit of unexplained correlation between them, however since the VIF is under ten I decided that I can keep them.  


```{r, echo=TRUE, eval=TRUE}
correlationvars <- c(3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,24,25)
completecases <- final_complete[complete.cases(final_complete),]

basemodel2 <- lm(totalcostinstate ~ rank + acceptedenrolled +  studentpopulation * amountofapplicants + submitttingsat +
                  studenttofacultyratio * fouryeargrad +  onfinancialaid * recevingloan + fulltime + female + white +
                  averageloan * averagegrant + applicationfee + state + 
                  sat25 * admitted + calendar+ setting + athlete + sportsrevenue, data=completecases)

#stepwise function commented out for R-markdown file.  
#step(basemodel2,direction="both")

bestbackward<- lm(formula = totalcostinstate ~ acceptedenrolled + amountofapplicants + 
                    studenttofacultyratio + fouryeargrad + onfinancialaid + averageloan + 
                    averagegrant + sat25 + admitted + athlete + rank + studenttofacultyratio:fouryeargrad + 
                    averageloan:averagegrant + sat25:admitted, data = final_complete, na.action=na.omit)


AIC(allmodel, basemodel, basenostate, bestbackward)

vif(bestbackward)
vif(basenostate)

#qqPlot(basenostate, main="QQ Plot")
#resid <- resid(basenostate)

par(mfrow=c(2,2))
plot(basenostate)
```


#PCA Analysis

I went on to do some principle component analysis.  I created two models a principle component regression and a principle least squared regression.  From the summary of the two different models I concluded that the plsr model was the better model since it accounted for more variance of tuition using fewer components.

```{r, echo=TRUE, eval=TRUE}
temp1 <- final_complete[,correlationvars]
temp2 <- complete.cases(temp1)
complete_continous <- temp1[temp2,]

pcrModel = pcr(totalcostinstate ~ ., 
               data=complete_continous, rotation='Varimax')

plsrModel = plsr(totalcostinstate ~ ., 
                 data=complete_continous, rotation='Varimax')

summary(pcrModel)
summary(plsrModel)
```

Looking at the components in the plsr model does not provide as clear a division in the variable groups as I had hoped it would.  The first three components account for 85% of the variance however there are no heavily weighted variables in those components.  

```{r, echo=TRUE, eval=TRUE}
loadings <- as.data.frame(plsrModel[1])
names(loadings) <- c("PC1", "PC2", "PC3", "PC4", "PC5",
                     "PC6", "PC7", "PC8", "PC9", "PC10",
                     "PC11", "PC12", "PC13", "PC14", "PC15",
                     "PC16", "PC17", "PC18", "PC19", "PC20")              
loadings[,1:4]
```

#Future Work

Using this plsr model I intend to check which schools are over-priced and which ones are good deals in relation to the variables that I have found to be significant.  

